{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrect (Viterbi)\n",
    "#### Authors:\n",
    "v1.0 (2017 Spring) Tavor Baharav, Kabir Chandrasekhar, Sinho Chewi, Andrew Liu, Kamil Nar, David Wang, and Kannan Ramchandran\n",
    "\n",
    "v1.1 (2017 Fall) Sinho Chewi, Avishek Ghosh, Chen Meng, Abhay Parekh, and Jean Walrand\n",
    "\n",
    "v1.2 (2018 Spring) Tavor Baharav, Kaylee Burns, Gary Cheng, Sinho Chewi, Hemang Jangle, William Gan, Alvin Kao, Chen Meng, Vrettos Muolos, Kanaad Parvate, Ray Ramamurti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model\n",
    "\n",
    "In this lab you will be implementing autocorrect using the Viterbi algorithm.\n",
    "\n",
    "We will model the English language as a HMM. The state space is the set of all English words. The *hidden states* are the intended words of the writer, while the *emissions* are the actual words written down, which include typos. As an example, suppose an author intends to write the sentence \"the dog barks\". The sequence of hidden states is\n",
    "\n",
    "$$X(0) = \\text{the},$$\n",
    "$$X(1) = \\text{dog},$$\n",
    "$$X(2) = \\text{barks}.$$\n",
    "\n",
    "However, the author is prone to making errors while typing, so the emissions (sometimes called observations) are\n",
    "\n",
    "$$Y(0) = \\text{the},$$\n",
    "$$Y(1) = \\text{dog},$$\n",
    "$$Y(2) = \\text{barjs}.$$\n",
    "\n",
    "The Viterbi algorithm gives the most likely sequence of hidden states, i.e. the most likely sequence of intended words which produced the typo-filled output text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Model\n",
    "The transition model gives the probability of the transition from the hidden state $x(i)$ to the hidden state $x(i+1)$. \n",
    "\n",
    "In the field of natural language processing (NLP), the literature often mentions *$n$-grams* models. A *unigram* is a single word. A *bigram* is a pair of words which appear consecutively in a text. You will use the unigram counts to determine the relative frequencies of words. The word frequencies specify the initial distribution of the HMM. The bigram counts are used to infer word-to-word transition probabilities.\n",
    "\n",
    "We will be using data from the source http://norvig.com/ngrams/ for the unigram and bigram counts. We took the unigrams from the file \"count_1w100k.txt\" and the bigrams from the file \"count_2w.txt\".\n",
    "\n",
    "You will be building a list of initial distribution of 10000 words, and the word-to-word transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing everything here\n",
    "import math\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to speed up the process, we limit the number of words to a small subset of the dataset, based on the most frequently appearing words.\n",
    "# We chose to limit the number of words to 10,000, but you can play around with this number too.\n",
    "NUM_WORDS = 10000\n",
    "\n",
    "# Build the initial distribution of the HMM\n",
    "# i.e. the count of each word divided by the sum of counts of all words\n",
    "pi_0 = np.zeros(NUM_WORDS)\n",
    "# key: word, value: an integer ID of the word\n",
    "word_to_id = {}\n",
    "# a list of all the words\n",
    "word_list = []\n",
    "\n",
    "\n",
    "\n",
    "with open(\"count_1w100k.txt\") as f:\n",
    "    for i in range(NUM_WORDS):\n",
    "        line = f.readline()\n",
    "        tokens = line.lower().split()\n",
    "        word = tokens[0]\n",
    "        count = float(tokens[1])\n",
    "        \n",
    "        #Your code here\n",
    "        # Construct pi_0, word_to_id and word_list\n",
    "\n",
    "\n",
    "pi_0 = pi_0 / np.sum(pi_0)\n",
    "log_pi_0 = np.log(pi_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the word-to-word transition matrix\n",
    "# this process is similar to what you did in MCMC lab (except you used \"War and Peace\")\n",
    "P = np.zeros((NUM_WORDS, NUM_WORDS))\n",
    "\n",
    "\n",
    "with open(\"count_2w.txt\") as f:\n",
    "    for line in f:\n",
    "        tokens = line.lower().split()\n",
    "        word1 = tokens[0]\n",
    "        word2 = tokens[1]\n",
    "        count = float(tokens[2])\n",
    "        if word1 in word_to_id and word2 in word_to_id:\n",
    "            #Your code here\n",
    "            # Construct P\n",
    "\n",
    "\n",
    "# log_P is your transition distance matrix\n",
    "P = P / np.sum(P)\n",
    "log_P = np.log(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity Checks\n",
    "# probability of seeing 'probability' is much smaller than probability of seeing 'the'\n",
    "print(\"Probability of 'the'\", pi_0[word_to_id[\"the\"]])\n",
    "print(\"Probability of 'probability'\", pi_0[word_to_id[\"probability\"]])\n",
    "\n",
    "\n",
    "# probability of 'you' followed by 'is' should be much smaller than Probability of 'you' followed by 'are'\n",
    "print(\"Probability of 'you' followed by 'are'\", P[word_to_id[\"you\"], word_to_id[\"are\"]])\n",
    "print(\"Probability of 'you' followed by 'is'\", P[word_to_id[\"you\"], word_to_id[\"is\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission Model: Edit Distance\n",
    "\n",
    "The emission model gives the probability of seeing a word $y(i)$ when the true hidden state is $x(i)$. The emission probabilities are based on a measure of string dissimilarity known as *edit distance* or *Levenshtein distance*.\n",
    "\n",
    "$\\underline{Inputs}$ Two strings s_1[1...n], s_2[1...m]\n",
    "\n",
    "$\\underline{Goal}$ Number of characterwise Inserts, Deletes and Substitutes required to edit s_1[1...n] into s_2[1...m]\n",
    "\n",
    "$\\underline{Definition}$\n",
    "The edit distance between two strings $s_1$ and $s_2$ (with lengths $m$ and $n$ respectively) is defined by the recursive equations\n",
    "\n",
    "$$d(s_1, s_2) =\n",
    "\\begin{cases}\n",
    "  m & \\text{if } s_2 \\text{ is empty}, \\\\\n",
    "  n & \\text{if } s_1 \\text{ is empty}, \\\\\n",
    "  \\min\n",
    "  \\begin{cases}\n",
    "    d(s_1(2, \\dotsc, m), s_2) + 1 \\\\\n",
    "    d(s_1, s_2(2, \\dotsc, n)) + 1 \\\\\n",
    "    d(s_1(2, \\dotsc, m), s_2(2, \\dotsc, n)) + \\mathbf{1}\\{s_1(1) \\ne s_2(1)\\}\n",
    "  \\end{cases}\n",
    "  & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "The notation is as follows: $s_1(1)$ represents the first character in the first string; and $s_1(2, \\dotsc, n)$ represents the substring which starts at the second character and ends at the $n$th character of $s_1$. Similarly for $s_2$.\n",
    "\n",
    "Here is some $\\underline{intuition}$ for edit distance: it represents the minimum number of operations to change $s_1$ into $s_2$, where an operation is either\n",
    "- adding a new character,\n",
    "- deleting an existing character,\n",
    "- or changing one character into another.\n",
    "\n",
    "You can simply think of edit distance as a way to measure the distance between two strings. An example is shown below, demonstrating that the edit distance between \"elephant\" and \"relevant\" is 3, which is achieved by taking \"relevant\", deleting the r, replacing the v with a p, and adding an h immediately after. Convince yourself that the following matrix makes sense. Fill it out top to bottom, left to right.\n",
    "<img src = \"https://hsto.org/storage2/74b/c0f/a85/74bc0fa858652701ff47bfd125c83eeb.png\">\n",
    "\n",
    "[This link](https://nlp.stanford.edu/IR-book/html/htmledition/edit-distance-1.html) may be a helpful resource for understanding implementing the edit distance. It also includes a pseudo code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function that takes in two words and return the edit distance between them.\n",
    "# Helper function used to calculate the emission probability\n",
    "def edit_distance(word1, word2):\n",
    "    m = len(word1)\n",
    "    n = len(word2)\n",
    "    d = np.zeros((m + 1, n + 1))\n",
    "    for i in range(m + 1):\n",
    "        # Your code here\n",
    "    for j in range(1, n + 1):\n",
    "        # Your code here\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            # Your code here\n",
    "    return d[m, n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just implemented the idea of dynamic programming in edit_distance function! Dynamic programming is an algorithm that uses a table to store intermediate values as it builds up the probability of the observation sequence. A concrete example of a dynamic programming problem is to find the longest path in a DAG (directed acyclic graph). Viterbi algorithm is itself another example of dynamic programming. You start from the end of the sequence and find the node that gives the shortest distance at each level moving backward. At each level, you assume you have the info about the shortest distance from each node at the next level to the end of the sequence, and you use that to calculate the shortest distance for the current level. This is exactly dynamic programming!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emission model is given by a Poisson distribution based on the edit distance. If the hidden state $x$ and the emission $y$ have edit distance $d(x, y) = k$, then the emission probability is\n",
    "\n",
    "$$Q(x, y) = \\frac{e^{-\\lambda} \\lambda^k}{k!}.$$\n",
    "\n",
    "The larger the edit distance, the smaller the emission probability, which aligns with our intuition. \n",
    "\n",
    "Implement a function that gives matrix log_Q = log(P), where P is the matrix of emission probability based on the emission model above. log_Q is of dimension (n, m), where n is the number of words in the sentence, and m is the number of potential words for decoding. You may want to play around with different values of $\\lambda$ for the poisson distribution to see which one works best. We tried using $\\lambda = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try play around with this number\n",
    "RATE = 0.01\n",
    "\n",
    "# Words is a list of lower-case words in the sentence we are trying to decode/autocorrect\n",
    "# n: the number of words in the sentence\n",
    "# m: the number of potential words for decoding\n",
    "# log_Q is your emission distance matrix\n",
    "def emission_distance_matrix(words): \n",
    "    count = len(words)\n",
    "    log_Q = np.zeros((count, NUM_WORDS))\n",
    "    for i in range(count):\n",
    "        word = words[i]\n",
    "        for j in range(NUM_WORDS):\n",
    "            dist = #Your code here\n",
    "            log_Q[i, j] = #Your code here\n",
    "    return log_Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Viterbi\n",
    "\n",
    "Now that you have a function for log emission distance matrix, a list of initial distribution of 10000 words, and the word-to-word transition matrix, it's finally time to implement the viterbi algorithm to do some autocorrect! Try to use NumPy's functions whenever possible because they are *vectorized* (optimized for high performance).\n",
    "\n",
    "Good luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(sentence):\n",
    "    words = sentence.lower().split()\n",
    "    count = len(words)\n",
    "    \n",
    "    log_Q = emission_distance_matrix(words)\n",
    "\n",
    "    # V[i,j] denotes the min distance of seeing word j at position i\n",
    "    V = np.zeros((count, NUM_WORDS))\n",
    "    # pointers[i,j] denotes which word to go to at position i+1 to achieve the min distance of seeing word j at position i\n",
    "    pointers = np.zeros((count - 1, NUM_WORDS))\n",
    "\n",
    "    \n",
    "    for i in range(count - 2, -1, -1):\n",
    "        for j in range(NUM_WORDS):\n",
    "            #Your code here\n",
    "            # Construct V and pointers\n",
    "\n",
    "    start_state = np.argmin(V[0] - log_pi_0 - log_Q[0])\n",
    "    sentence = [word_list[int(start_state)]]\n",
    "    curr_state = start_state\n",
    "    \n",
    "    for i in range(count - 1):\n",
    "        #Your code here\n",
    "        # Construct sentence\n",
    "\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the final result!\n",
    "# Staff solution returns \"why is the sky blue\",\n",
    "# takes under 4 seconds to run\n",
    "start = time.time()\n",
    "print(viterbi(\"whyy is th ski bluui\"))\n",
    "end = time.time()\n",
    "print(\"Execution Time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.cs.sfu.ca/~mori/courses/cmpt310/a5.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
