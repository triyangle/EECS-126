{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: Because of limited dimensions, this diagram inaccurately suggests that increasing the length of $V(n-1)$ could bring $X(n)$ closer to span $Y^n$ and that $V(n-1)$ and $W(n)$ aren't orthogonal. However, it may still help you understand is how the formulas for $\\hat{\\ X}(n)$, $S_n$, $K_n$, and $\\Sigma_n$ are derived.\n",
    "\n",
    "<img src=\"diagram.png\" style=\"width:480px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing the Algorithm for the Scalar Case \n",
    "\n",
    "The idea behind the Kalman Filter is that we should base our estimate $\\hat{\\ X}(n)$ on a reasonable value determined by our estimate $\\hat{\\ X}(n-1)$ from previous observations $Y^{n-1}$ and our latest observation $Y(n)$. How that is actually done is best explained in linear algebra terms.\n",
    "\n",
    "If you are familar with the Gram-Schmidt process, the Kalman Filter is just calcuating projections with that.\n",
    "\n",
    "In the diagram you can see a plane with axes span $Y^{n-1}$ and span $\\tilde{\\ Y}(n)$. What this plane represents is all random variables that can be represented as a linear combination (hence LLSE) of the first $n$ observations, which are random variables themselves. $X(n)$ may not be directly representable as such a linear combination, but we can try to find a $\\hat{\\ X}(n)$ in this plane that minimizes the $\\| X(n) - \\hat{\\ X}(n) \\|^2 = \\mathbb{E}[(X(n)-\\hat{\\ X}(n))^2]$\n",
    "\n",
    "To do this, we project $X(n)$ onto this plane. But since span $Y^{n-1}$ and $Y(n)$ aren't necessarily orthogonal (which makes calculating projections difficult), we need to use the idea behind Gram-Schmidt to find a new vector $\\tilde{\\ Y}(n)$ that is orthogonal to span $Y^{n-1}$.\n",
    "\n",
    "Before we get to that, let's first assume we can find a $\\tilde{\\ Y}(n)$ in which case the projection onto span $Y^n$ is the sum of the projection onto span $Y^{n-1}$ and $\\tilde{\\ Y}(n)$. One thing we know is that we've already calculated $\\hat{\\ X}(n-1)$, the projection of $X(n-1)$ onto span $Y^{n-1}$, as shown in the diagram. Since $AX(n-1)$ simply scales $X(n-1)$ and $V(n-1)$ is orthogonal to span $Y^{n-1}$, it makes sense that the projection of $X(n)$ onto span $Y^{n-1}$ is just $\\mathbf{A \\hat{\\ X}(n-1)}$.\n",
    "\n",
    "Ok, let's now try to find $\\tilde{\\ Y}(n)$. Gram-Schmidt tells us its just $Y(n)-\\hat{\\ Y}(n \\mid n-1)$, where $\\hat{\\ Y}(n \\mid n-1)$ is the projection of $Y(n)$ onto span $Y^{n-1}$. By the same principle used to derive $\\hat{\\ X}(n \\mid n-1)$, we argue that $Y(n)-\\hat{\\ Y}(n \\mid n-1) = \\mathbf{Y(n)-CA\\hat{\\ X}(n-1)}$.\n",
    "\n",
    "Now the tricky part comes in, projecting $X(n)$ onto $\\tilde{\\ Y}(n)$, which is why we come up with some \"helper variables\" $S_n$, $K_n$, and $\\Sigma_n$. First, however, we argue that projecting $X(n)$ onto $\\tilde{\\ Y}(n)$ is the same as projecting $Z(n)$ onto $\\tilde{\\ Y}(n)$, because $Z(n)$ just gets rid of a component of $X(n)$ that is orthogonal to $\\tilde{\\ Y}(n)$.\n",
    "\n",
    "By the formula for projection, it's equal to $\\frac{\\mathbb{E}[Z(n) \\tilde{\\ Y}(n)]}{\\mathbb{E}[\\tilde{\\ Y}(n)^2]} \\tilde{\\ Y}(n)$. We define $K_n$ to be the coefficient that multiplies $\\tilde{\\ Y}(n)$. $\\mathbb{E}[\\tilde{\\ Y}(n)^2]$ is the easier one to calculate. First let's define the squared length of $Z(n)$ to be $S_n$. Now, if you believe that $Z(n)$ and $W(n)$ are orthognal ($W(n)$ is the segment connecting $CX(n)$ and $Y(n)$, but because we can sort of shift vectors it's identical to the one connecting $CZ(n)$ to the axis span $\\tilde{\\ Y}(n)$), then the squared length of $\\tilde{Y}(n)$, by Pythagoras' theorem, is $\\| C Z(n) \\|^2 + \\| W(n) \\|^2 = C^2 \\| Z(n) \\|^2 + \\| W(n) \\|^2 = \\mathbf{C^2 S_n + W_n}$. They are indeed orthogonal, because $Z(n)$ is a linear function of $X(0)$, $V(0), \\dots, V(n-1)$, $W(0),\\dots,W(n-1)$, all of which are orthogonal to $W(n)$.\n",
    "\n",
    "How do we calculate $S_n$ though? Another thing to notice is that $Z(n) = AX(n-1)-\\hat{\\ X}(n \\mid n-1) + V(n-1)$ (remember we can sort of just shift vectors), and furthermore, $AX(n-1)-\\hat{\\ X}(n \\mid n-1)$ and $V(n-1)$ are orthogonal. So by Pythagoras, $S_n = \\mathbf{A^2 \\Sigma_{n-1}+\\Sigma_V}$.\n",
    "\n",
    "Now for $\\mathbb{E}[Z(n) \\tilde{\\ Y}(n)]$. One thing we know about inner products is that its actually equal to $\\| Z(n) \\| \\| \\tilde{\\ Y}(n) \\| \\cos(\\theta)$, where $\\theta$ is the angle between the two vectors. If we look at the diagram, $\\| \\tilde{\\ Y}(n) \\| \\cos(\\theta)$ is $\\| C Z(n) \\|$, so what we are calculating is $C\\| Z(n) \\|^2$, which is $\\mathbf{CS_n}$. Finally, we can conclude that $\\mathbf{K_n = \\frac{CS_n}{C^2 S_n + \\Sigma_W}}$.\n",
    "\n",
    "So $\\mathbf{\\hat{\\ X}(n) = A\\hat{\\ X}(n-1) + K_n (Y - CA\\hat{\\ X}(n))}$\n",
    "\n",
    "We still need some clean up with $\\Sigma_n$. However, by Pythagoras, it's just $\\| Z(n) \\|^2 - \\| K_n \\tilde{\\ Y}(n) \\|^2$, which you can verify is $S_n - K_n C S_n$, leading us to the formula $\\mathbf{(1-K_nC)S_n}$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
