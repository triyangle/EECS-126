{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11 (Option 1) - Viterbi\n",
    "\n",
    "*This lab has been iteratively developed for EE 126 at UC Berkeley by Rishi Sharma, Sahaana Suri,  Paul Rigge, Kangwook Lee, Kabir Chandrasekher, Max Kanwal, Tony Duan, David Marn, Ashvin Nair, Tavor Baharav, Sinho Chewi, Andrew Liu, Kamil Nar, David Wang, and Kannan Ramchandran*\n",
    "\n",
    "v1.1 (2018 Spring) Tavor Baharav, Kaylee Burns, Gary Cheng, Sinho Chewi, Hemang Jangle, William Gan, Alvin Kao, Chen Meng, Vrettos Muolos, Kanaad Parvate, Ray Ramamurti\n",
    "\n",
    "Special thanks to David Tse and the teaching staff of EE178 at Stanford for their modifications to this lab.\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "Alice and Bob are working together to bake some treats.\n",
    "Alice is in charge of sending Bob the recipe, and Bob is responsible for following directions.\n",
    "Bob is *extremely* literal- if an error in the recipe says to use 2000 eggs instead of 2, he will use 2000 eggs without a second thought.\n",
    "Alice is aware of Bob's lack of common sense, but she is also busy.\n",
    "She already has the recipe open on her phone, so she wants to send it via email (using wifi) to Bob.\n",
    "Unfortunately, Alice's evil next-door-neighboor Eve has her microwave running continuously at maximum power.\n",
    "Microwaves emit a lot of radiation around wifi's 2.4GHz channels, causing interference.\n",
    "This lab will explore different techniques for ensuring that Alice's message will make it to Bob uncorrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "We assume that Alice's message is $N$ bits long with each bit iid Bernoulli(0.5).\n",
    "We model the channel as a [binary symmetric channel](http://en.wikipedia.org/wiki/Binary_symmetric_channel), as shown below.\n",
    "Each bit sent through the channel is flipped (independently) with probability p.\n",
    "We'll assume p=0.05, which is a fairly typical value for wireless communications.\n",
    "\n",
    "![Alt](http://upload.wikimedia.org/wikipedia/commons/8/8e/Binary_symmetric_channel_%28en%29.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Coding\n",
    "It should make intuitive sense that we can get to low probabilities of error if we increase the number of repetitions.\n",
    "However, increasing the number of repetitions means we are lowering Alice's effective datarate.\n",
    "Alice's phone also has limited battery life, and sending lots of copies of the same data consumes a lot of energy.\n",
    "Is it possible to correct a lot of errors without having to send so many extra bits? \n",
    "\n",
    "Yes! The 802.11 standards for wifi use convolutional codes and LDPC codes to correct errors.\n",
    "Convolutional codes can be efficiently decoded using the Viterbi algorithm, so they will be the focus of this lab.\n",
    "<img src=\"http://imgur.com/BzlsY89.png\" width=\"500px\"></img>\n",
    "\n",
    "The above picture is a block diagram for a simple convolutional encoder.\n",
    "The input message is treated as a stream of bits \n",
    "\n",
    "$$d_0, d_1, d_2, d_3 \\ldots$$\n",
    "\n",
    "The input is shifted through a series of delays - at time k, the input is $d_k$, the first delay element (the \"D\" on the left) contains $d_{k-1}$, and the last delay elements (the \"D\" on the right) contains $d_{k-2}$.\n",
    "In this example, each input bit produces two output bits - the first output computed by the top \"adder\" and the second output computed by the bottom \"adder\". We denote the output of the top adder as $u_k$ and the bottom adder as $v_k$. The equations for each are given by\n",
    "\n",
    "\\begin{align*}\n",
    "u_k &= d_k + d_{k-2} &\\text{mod } 2\\\\\n",
    "v_k &= d_k + d_{k-1} + d_{k-2} &\\text{mod } 2\n",
    "\\end{align*}\n",
    "\n",
    "We further define $d_{-2}= d_{-1}=0$ for initalization of the algorithm. The two outputs are interleaved into one bitstream so the output is $(u_0, v_0, u_1, v_1, u_2, v_2, ...)$.\n",
    "\n",
    "The first thing to note is that this is not actually all that different from repetition coding.\n",
    "Like repetition coding, we are adding redundancy by generating multiple output bits per input bit.\n",
    "However, unlike repetition coding, convolutional codes have *memory*.\n",
    "Each output bit is a function of multiple input bits.\n",
    "The idea is that if there is an error, you should be able to use the surrounding bit estimates to help you figure out what was actually sent.\n",
    "\n",
    "The figure below shows the state transition diagram corresponding to the example encoder above.\n",
    "Each transition is labelled $d_k/(u_k, v_k)$.\n",
    "The two bits inside the circle correspond to the *state*. In order to compute the next output we must have $d_{k-2},d_{k-1}$ so we represent our state as two numbers $d_{k-2}d_{k-1}$ (of which there are 4 different combinations). \n",
    "\n",
    "As an example, consider the state $10$ and the transition $1/00$. The state is denoted in the format of  ($d_{k-2}$  $d_{k-1}$). The transition is denoted in the format of ($d_{k}/u_{k}v_{k}$). We first identify $d_{k-2} = 1$ and $d_{k-1}=0$ from the state. From the transition information we identify $d_k = 1$, $u_k = 0$, and $v_k = 0$ (note that $u_k$ and $v_k$ need not be given as they are entirely determined by the other three variables). The new state is then $d_{k-1}d_k = 10$ which is consistent with the diagram.\n",
    "\n",
    "Be sure to convince yourself that the encoder above is equivalent to the state transition diagram below.\n",
    "\n",
    "<img src=\"http://imgur.com/W9fSfpn.png\" width=\"300px\"></img>\n",
    "\n",
    "If we assume that the input bits are iid Bernoulli(0.5), this is a Markov chain with every state equally likely.\n",
    "We can run the Viterbi algorithm on our output bits (even after going through a noisy channel) to recover a good estimate of the input bits.\n",
    "\n",
    "We assume that we start at state 00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 The Convolutional Encoder\n",
    "### (A) Implement a general encoder, that encodes bits like above, for arbitrary generators. Your code will go in the encode function. \n",
    "*Hint*: Try to understand what the function apply_generator does.  It is helpful to go through some examples by hand before you start coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "# Takes in d_{k},d_{k-1},d_{k-2} as the 'state' and computes the output (a 1 or 0) \n",
    "# by XORing the bits specified by generator. \n",
    "# The generator basically tells the function whether to compute u_k or v_k in the above block diagram\n",
    "def apply_generator(state, generator):\n",
    "    return functools.reduce(lambda x,y: x^y, map(lambda x: x[0]*x[1], zip(state,generator)), 0)\n",
    "\n",
    "example_generators=[[1,0,1], [1,1,1]]  # Specifies the coefficients to be used in the output of our convolutional code\n",
    "\n",
    "# Given a bit stream, performs the operations in the block diagram above\n",
    "def encode(bits_in, generators=example_generators):\n",
    "    \"\"\"\n",
    "    >>> encode([1,1,0,1,0])\n",
    "    [1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n",
    "    >>> encode([0,1,0,1,0])\n",
    "    [0, 0, 1, 1, 0, 1, 0, 0, 0, 1]\n",
    "    \"\"\"\n",
    "    l = max(map(len, generators))\n",
    "    bits_in = list(bits_in) \n",
    "    state = [0]*l\n",
    "    output = []\n",
    "    for b in bits_in: \n",
    "        # Turns state d_{k-1},d_{k-2},d_{k-3} into d_{k},d_{k-1},d_{k-2} when new bit arrives \n",
    "        # Use state to add u_k and v_k to the output\n",
    "        # Your code here\n",
    "        \n",
    "        \n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 1, 1, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Check encode here\n",
    "print(encode([1,1,0,1,0]))\n",
    "print(encode([0,1,0,1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Mathematical Preliminaries\n",
    "In this section, we will walk through some theory behind the Viterbi algorithm.  You will not need to derive any of it, but this will help you understand the algorithm in a big picture sense. \n",
    "\n",
    "We start by defining the random variables associated with the problem. We let $X_0,X_1,X_2\\ldots$ be the binary random variables which are input into the encoder. We further define the random variables $U_0,U_1,\\ldots$ and $V_0,V_1,\\ldots$ to the result of the upper and lower branch of the convolutional encoding of $X_0,X_1,\\ldots$. Lastly, we let $Y_0^u,Y_1^u,\\ldots$ be the output when $U_0,U_1,\\ldots$ are passed through the binary symmetric channel. We define $Y_0^v,Y_1^v,\\ldots$ analogously for the $V_i$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (A) We will first show that if the $X_i$'s are i.i.d. and equally probable 0 or 1, then the MAP rule\n",
    "\n",
    "$$\\arg \\max_{d_0,\\ldots,d_n} P\\bigl(X_0=d_0,\\ldots, X_n = d_n \\bigl{|} Y^u_0 =a_0, Y_0^v = b_0,Y^u_1 =a_1, Y_1^v = b_1,\\ldots,Y^u_n =a_n, Y_n^v = b_n\\bigr)$$\n",
    "\n",
    "### is equivalent to the Maximum Likelihood rule\n",
    "\n",
    "$$\\arg \\max_{d_0,\\ldots,d_n} P\\bigl(Y^u_0 =a_0, Y_0^v = b_0,Y^u_1 =a_1, Y_1^v = b_1,\\ldots,Y^u_n =a_n, Y_n^v = b_n\\big{|}X_0=d_0,\\ldots, X_n = d_n\\bigr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of notation we'll let $\\tilde{Y} = (a_0, b_0, a_1, b_1,\\ldots, a_n, b_n)$ and $\\tilde{X} = (d_0,\\ldots, d_n)$.\n",
    "\n",
    "Now we can rewrite the MAP estimate using Baye's Rule as\n",
    "$$\\arg \\max_{d_0,\\ldots,d_n} P(\\tilde{X}|\\tilde{Y}) = \\arg \\max_{d_0,\\ldots,d_n} \\frac{P(\\tilde{Y}|\\tilde{X})P(\\tilde{X})}{P(\\tilde{Y})}$$\n",
    "\n",
    "Now since the bits are equiprobable and the denominator is constant with respect to the variables over which we are maximizing, this reduces to \n",
    "\n",
    "$$\\arg \\max_{d_0,\\ldots,d_n} P(\\tilde{Y} |\\tilde{X})$$\n",
    "\n",
    "which was to be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) We will now show that if $Z_1$, $Z_2$ and $Z_3$ are random variables such that $Z_1$ and $Z_3$ are independent conditional on $Z_2$ and $Z_2$ is a function of $Z_1$ (i.e. $Z_2 = g(Z_1)$) then \n",
    "\n",
    "$$P\\bigl(Z_3 = a \\big{|} Z_1 = b\\bigr) = P\\bigl(Z_3 = a \\big{|} Z_2 = g(b)\\bigr)$$\n",
    "\n",
    "We note that:\n",
    "\\begin{align*}\n",
    "P(Z_3 = a|Z_1=b) &= P(Z_3 = a|Z_1 = b, Z_2 = g(Z_1))\\\\\n",
    "&= P(Z_3 = a|Z_1 = b, Z_2 = g(b))\\\\\n",
    "&= P(Z_3 = a|Z_2 = g(b))\n",
    "\\end{align*}\n",
    "Where the last line uses the conditional independence property.We note that:\n",
    "\\begin{align*}\n",
    "P(Z_3 = a|Z_1=b) &= P(Z_3 = a|Z_1 = b, Z_2 = g(Z_1))\\\\\n",
    "&= P(Z_3 = a|Z_1 = b, Z_2 = g(b))\\\\\n",
    "&= P(Z_3 = a|Z_2 = g(b))\n",
    "\\end{align*}\n",
    "Where the last line uses the conditional independence property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C) Next, using result (B) or otherwise, we would like to show that the MAP expression in (A) is equivalent to:\n",
    "\n",
    "$$\\arg \\max_{d_0,\\ldots,d_n} P\\bigl(Y^u_0 =a_0, Y_0^v = b_0,Y^u_1 =a_1, Y_1^v = b_1,\\ldots,Y^u_n =a_n, Y_n^v = b_n \\big{|} U_0=u_0, V_0 =v_0\\ldots, U_n = u_n, V_n=v_n\\bigr)$$\n",
    "\n",
    "### where $(u_0,v_0,\\ldots,u_n,v_n)$ is the output stream corresponding to the input stream.\n",
    "\n",
    "We let $Z_1 = (X_0,X_1,\\ldots, X_n)$, $Z_2 = g(Z_1)$, and $Z_3 = (Y^u_0, Y_0^v, Y^u_1, Y_1^v,\\ldots,Y^u_n, Y_n^v) $ where $g(\\cdot)$ is the function peformed by the block diagram above. The result then follows from (A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convince yourself that the maximization in result (C) can be solved using the Viterbi algorithm on the Markov chain above with an appropriate metric associated with each transition in the chain (branch metric). Try to think of an explicit expression for the branch metric. (You do not need to turn anything in, this is just for you to think about and it will help in the next part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Q2 The Viterbi Algorithm\n",
    "An implementation for a Viterbi decoder can be constructed as follows:\n",
    "1. A state $s$ has a *path metric* $p_s$ that gives the number of observed bit errors associated with being in $s$ at a given time.\n",
    "2. A state $s$ and input bit $b$ have *branch metric* $b_{s,b}$ that compares the observed channel output to the expected output given that you were in state $s$ and had input bit $b$. The branch metric is the number of different bits between the observed and expected output (Hamming weight).\n",
    "3. If input $b$ has transitions from state $s$ to $s'$, we can compute an updated path metric as $p_s + b_{s,b}$.\n",
    "4. Each state $s'$ will have two incoming transitions, we select the minimum  $p_s+b_{s,b}$ and call that our new path metric $p_s'$. This is called Add-Compare-Select.\n",
    "5. Traceback uses the decisions at each add-compare-select to reconstruct the input bit sequence.\n",
    "Starting at the ending state with the smallest path metric, traceback finds the predecessor based on the decision made by the add-compare-select unit.\n",
    "For example, if at the end, state zero has the lowest path metric, and the last add-compare-select for state zero chose the path_metric coming from state 2, we know that the last input bit was zero and the previous state was 2. This continues backwards until it reaches the beginning.\n",
    "\n",
    "[This link](http://home.netcom.com/~chip.f/viterbi/algrthms2.html) may be a helpful resource for understanding implementing the Viterbi algorithm for convolutional codes (note that what we are designing is called a hard decoder- soft decoders are a refinement that we won't worry about).\n",
    "\n",
    "We assume that we start and end in state 0.\n",
    "We end in state 0 by appending enough 0s to the end of our input to force us into 0.\n",
    "Our decoder relies on this by initializing all path metrics to a big number, except for 0 which we initialize to 0.\n",
    "### (A) In the space below, implement a viterbi decoder for such a convolutional encoder.  \n",
    "It should work for all generators, so it should also work on the 4 state encoder above.\n",
    "\n",
    "*Hint*: You may find the functions ham_dist and int2state defined below helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import inf, zeros, array\n",
    "import numpy as np\n",
    "\n",
    "# Returns the hamming distance of two binary vectors (must be of array type)\n",
    "def ham_dist(vec1, vec2):\n",
    "    return np.sum(np.logical_xor(vec1,vec2))\n",
    "\n",
    "# Return a list of 0 and 1 to denote the state\n",
    "def int2state(i, w):\n",
    "    return [i&(1<<k)>0 for k in range(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(bits_in, generators=example_generators):\n",
    "    num_states = 1 << (max(map(len, generators))-1) # 2^num delays\n",
    "    states = [int2state(i, len(generators)-1) for i in range(num_states)]\n",
    "    # path metrics store the probability of being in a state\n",
    "    path_metrics = {}\n",
    "    # pointers pointing to the surviving predecessor states\n",
    "    tb = {}\n",
    "    \n",
    "    # We start from state 00\n",
    "    path_metrics[0] = 0\n",
    "    tb[0]=[]\n",
    "    for i in range(1, num_states):\n",
    "        path_metrics[i] = 1.e10\n",
    "        tb[i] = []\n",
    "        \n",
    "    for i in range(int(len(bits_in)/len(generators))):\n",
    "        observation = bits_in[len(generators)*i:len(generators)*(i+1)]\n",
    "        # a matrix with shape num_state * 2 (2 potential bits that can lead to the current state)\n",
    "        new_path_metrics = {}\n",
    "        for i in range(num_states):\n",
    "            new_path_metrics[i] = [0,0]\n",
    "        for i in range(num_states*2):\n",
    "            # Update the new_path_metrics matrix\n",
    "            # Path metric of current state = Path metric of previous state + Branch metric between observation and current state\n",
    "            # int2state and apply_generator might be helpful here\n",
    "            # Your code here\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        for i in range(num_states):\n",
    "            if new_path_metrics[i][0] < new_path_metrics[i][1]:\n",
    "                #Update path_metrics and tb\n",
    "                # Your code here\n",
    "                \n",
    "                \n",
    "                \n",
    "            else:\n",
    "                #Update path_metrics and tb\n",
    "                # Your code here\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    finish = min(path_metrics, key=path_metrics.get)\n",
    "    return traceback(tb,finish)[:len(bits_in)//len(generators)] # remove trailing zeros\n",
    "\n",
    "\n",
    "# Given the finish state and pointers in tb, return the decoded output\n",
    "def traceback(tb,finish):\n",
    "    state = finish\n",
    "    output = []\n",
    "    for i in range(len(tb[0])-1, -1, -1):\n",
    "        # Append the last bit in state to the output\n",
    "        # Shift the state one bit forward\n",
    "        # Your code here\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return output[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "After you have your code working for the 4-state example code, run the following code to make sure it works on the more complicated wifi codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test your code here\n",
    "wifi_generators = [ [1,0,1,1,0,1,1], [1,1,1,1,0,0,1] ]\n",
    "msg = (1,0,1,0,1,0,1,0)\n",
    "a = encode( msg, generators=wifi_generators)\n",
    "b=[i for i in bsc(a,p=0.05)]\n",
    "c=viterbi(b, generators=wifi_generators)\n",
    "print(msg)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) The following code block defines two functions, one which converts strings to their binary ASCII representations, and the other that converts from bits to ASCII. Use your implementation of the Viterbi Algorithm to decode the message in 'secret.txt' (which has been encoded using the convolutional code described in this lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#thanks to John Gaines Jr. and Simon Streicher from http://stackoverflow.com/questions/10237926/convert-string-to-list-of-bits-and-viceversa\n",
    "def tobits(s):\n",
    "    result = []\n",
    "    for c in s:\n",
    "        bits = bin(ord(c))[2:]\n",
    "        bits = '00000000'[len(bits):] + bits\n",
    "        result.extend([int(b) for b in bits])\n",
    "    return result\n",
    "\n",
    "def frombits(bits):\n",
    "    chars = []\n",
    "    for b in range(int(len(bits) / 8)):\n",
    "        byte = bits[b*8:(b+1)*8]\n",
    "        chars.append(chr(int(''.join([str(bit) for bit in byte]), 2)))\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fin = open('secret.txt')\n",
    "secret_bits = [int(i) for i in fin.read()[1:-1].split(', ')]\n",
    "secret_bits = np.array(secret_bits)\n",
    "fin.close()\n",
    "print(frombits(viterbi(secret_bits,generators=wifi_generators)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Q3.  Empirical Bit Error Rate of Convolutional Coding\n",
    "### (A) So how does the convolutional code compare with simple repetition coding? We are going to plot the bit error rate. \n",
    "\n",
    "$$ BER =\\frac{\\textrm{Number of incorrectly decoded bits}}{\\textrm{Total number of bits}}$$ \n",
    "\n",
    "### for some different channel parameters. For $0.01\\le p \\le 0.1$ on the x-axis, plot the bit error rate of the convolutional viterbi coding on a log scale on the y-axis. Run *100 trials* for randomly generated 512-bit long inputs at each channel parameter (this might take a little while to run). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Simulate the binary symmetric channel which corrupts the original bitstream with probability p\n",
    "def bsc(bits_in, p=0.05):\n",
    "    out = []\n",
    "    for b in bits_in:\n",
    "        if np.random.uniform() > p:\n",
    "            out.append(b)\n",
    "        else:\n",
    "            out.append(1-b)\n",
    "    return array(out)\n",
    "\n",
    "\n",
    "ps = np.linspace(0.01, 0.1, 10)\n",
    "def ber_experiment(p, n_trials = 100, n_bits = 512, generators=example_generators):\n",
    "    errors = 0\n",
    "    for i in range(n_trials):\n",
    "        bits_in = np.random.randint(0,2, 512)\n",
    "        # Your code here\n",
    "        # Construct received message and decoded message\n",
    "        # Calculate number of error bits\n",
    "        # bsc function might be helpful here\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    return float(errors) / (n_bits * n_trials)\n",
    "error_prob = [ber_experiment(p, generators=example_generators) for p in ps]\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.semilogy(ps, error_prob,'o')\n",
    "\n",
    "plt.title('Bit Error Rate vs. Crossover Probability of BSC')\n",
    "plt.xlabel('Crossover Probability (p)')\n",
    "plt.ylabel('Estimated BER')\n",
    "plt.ylim([10**(-4),10**(-1)])\n",
    "\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True,which = 'both',color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (B) Now plot the bit error rate for repetition encoding over a binary symmetric channel with p = 0.05 and p = 0.02. For $1\\le \\text{num_rep} \\le 6$  on the axis, plot the bit error rate on a log scale on the y_axis. Run *100 trials* for randomly generated 512-bit long inputs at each channel parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def ber_experiment_repetition(p, maxreps):\n",
    "        # Your code here\n",
    "        # Calculate the error rate for repetition encoding\n",
    "\n",
    "        \n",
    "        \n",
    "       \n",
    "    return -1\n",
    "\n",
    "max_reps = 6\n",
    "rep_error_prob = ber_experiment_repetition(0.02, max_reps)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.semilogy(range(1, max_reps+1), rep_error_prob,'o')\n",
    "\n",
    "plt.title('Bit Error Rate vs. Num Repetitions for p = 0.02')\n",
    "plt.xlabel('Num Repetitions')\n",
    "plt.ylabel('Estimated BER')\n",
    "plt.ylim([10**(-5),10**(-1)])\n",
    "\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True,which = 'both',color = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rep_error_prob = ber_experiment_repetition(0.05, max_reps)\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.semilogy(range(1, max_reps+1), rep_error_prob,'o')\n",
    "\n",
    "plt.title('Bit Error Rate vs. Num Repetitions for p = 0.02')\n",
    "plt.xlabel('Num Repetitions')\n",
    "plt.ylabel('Estimated BER')\n",
    "plt.ylim([10**(-3),1])\n",
    "\n",
    "plt.minorticks_on()\n",
    "plt.grid(b=True,which = 'both',color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (C) Assume we are communicating over a binary symmetric channel with $p =0.05$. Using our repetition encoder, what is the smallest number of repetitions $r$ we would need in order to achieve a superior BER to our convolutional code? Suppose the coded symbols are sent over a 10MHz wireless channel at the rate of $10\\times10^6$ symbols/sec. Compare the data rate, in bits per second (bps), of the convolutional code and the repetition code for this value of $r$. Now suppose, $p=0.02$. What is the minimum $r$ we need to outperform convolutional encoding? How does the data rate compare now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
